{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "<hr>\n# Classification\n<br>\n<br>\n<font size=5>In this case, the objective is to determinate if the variable belongs to one of several predefined classes or states. \nAn very known example of this is the classification of iris flowers among three species (setosa, versicolor or virginica) using as features measurements of length and width of sepals and petals.</font>\n<br>\n<br>\n<img src=\"pictures/iris.png\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>\n<font size=5> A calssification problem can be binary (two classes) or multi-class\n<br>\n<br>\n<img src=\"pictures/binaryandnot.png\" alt=\"drawing\" width=\"600\"/>\n<br>\n<br>\nWe will first focus on binary porblems and later we can expand this to multi-class porblems</font>\n\n## Losgistic regression\n<br>\n<font size=5>We will start with a simple classification algortim called logistic regression. Trough it we will undrestand some characteristics and specificities of classification algorithms.\n<br>\n<br>\nAs ususal, we will undrestand this method using an example.\n<br>\n<br>\nImagine you have a group of mouses and want to determine wether they are obese or not using their weight. You may have data that looks like this:\n<br>\n<br>\n<img src=\"pictures/logisticr1.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>\nIn this case we will classify mouses in two classes: obese an not obese.  That means that our output variable is not a continuos value, but a discrete quantity that may take only two values: 0 or 1. 0 means the mouse is not obese, 1 means it is obese.\n<br>\n<br>\nThe first thing we coold do with this is to aproach these as a regression porblem and try to fit a linear model to the data. Something like this:\n<br>\n<br>\n<img src=\"pictures/logisticr2.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>    \n And to cope with the fact that the output is dicrete we could defien that:\n<br>\n<br>\nif $y\\geq 0.5$ then $y=1$\n<br>\n<br>\nif $y < 0.5$ then $y=0$\n<br>\n<br>\nAn taht means taht we would have some sort of a threshold like this: \n<br>\n<br>\n<img src=\"pictures/logisticr3.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>\nAnd that result is probably not that bad.\n<br>\n<br>\nHow ever, it may have some porblems as we will se in the next example. Imagine we add an extra data point like this:\n<br>\n<br>\n<img src=\"pictures/logisticr4.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>    \nThat example shouldn´t change anything in terms of classification, but it can really damage our model:    \n<br>\n<br>\n<img src=\"pictures/logisticr5.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br> \nThe idea behind logistic regression is to change the model. We will not longer use a linear equation, but a losgistic or sigmoid equation.\n<br>\n<br>\n$y(x)=g(\\theta^Tx)$ \n<br>\n<br>\n$ g(z)= \\frac{1} {1+e^{-z}}$\n<br>\n<br>\nIt looks like this:\n<br>\n<br>\n<img src=\"pictures/logisticcurve.png\" alt=\"drawing\" width=\"500\"/>\n<br>\n<br>\nThis is going ot behave much better than the linear equiation and you can see that the output is necesarely goning to be between 0 and 1.\n<br>\n<br>\n## Interpretation of the output\n<br>\n<br>\nA commo way to interpret the output of this kind of model is that the value of the output represents the probability that the sample has to belong to a specific class. That is: \n<br>\n<br>\n$p(y=1|x;\\theta)$\n<br>\n<br>\nAnd, given that is a binary problem:\n<br>\n<br>\n$p(y=0|x;\\theta)= 1-p(y=1|x;\\theta)$\n<br>\n<br>\nAnd we could choose the threshold that defines wether a point belongs to a class or not, depending on how sensitive to detecting an specific class  .\n<br>\n<br>\n## Decision boundary\n<br>\n<br>\nIf we choose 0.5 as the threshold, we can see taht the output will be 1 if $g(z)>0.5$, that is if $z>0$ where $z=f(x)=\\theta^Tx$. that means taht we have a desicion boundary in f(x)=0.\n<br>\n<br>\n<img src=\"pictures/logisticcurve2.png\" alt=\"drawing\" width=\"500\"/>\n<br>\n<br>\nNow, let´s se how this looks if we have a calssification porblem with two input features. As an example we can imagine that we are going to calssify mouses in obese and not obese using their weight and size.\n<br>\n<br>\n<img src=\"pictures/logisticr6.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>\nNotice that in this representation we are not considering the output as an axis. We are representing it by the symbol of echa point.\n<br>\n<br>\nIf in this case $z= f(x)=\\theta^Tx=  \\theta_0+ \\theta_1x_1 + \\theta_2x_2$ and our decision boundary isin: \n<br>\n<br>\n$z=f(x)=\\theta_0+ \\theta_1x_1 + \\theta_2x_2=0$\n<br>\n<br>\nIn this plane, that equation correponds to a line, that can look something like this:\n<br>\n<br>\n<img src=\"pictures/logisticr7.jpg\" alt=\"drawing\" width=\"800\"/>\n<br>\n<br>\nHere we can clearly see that we have a linear decision boundary.\n<br>\n<br>\n## Cost function\nJust as in linear regression, we can use gradiende descent to solve this porblem, but the cost funtion will have to change. In linear regression we used:\n<br>\n<br>\n $ J(\\beta) = \\frac{1}{m} \\sum_{i=1}^m ( \\frac{1}{2} (f(x^{(i)}) -y^{(i)})^2)$    \n<br>\n<br>\nThat is\n<br>\n<br>\n $ J(\\beta) = \\frac{1}{m} \\sum_{i=1}^m (cost(f(x),y)$    \n<br>\n<br>\nNow, we will change he \"cost\". The reason to change it is that if we use the same cost $J(\\beta)$ is going to be a non convex function. So, to guarantee a convex funtion we will use the following.\n<br>\n<br>\n$cost(f(x),y) = \\begin{cases} -log(f(x)), & \\mbox{if } y\\mbox{ =1} \\\\ -log(1-f(x)), & \\mbox{if } y\\mbox{ =0} \\end{cases}$\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import linear_model",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name __check_build",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b94db01ee140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/home/nbuser/anaconda2_501/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name __check_build"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#from sklearn.datasets import make_classification\nfrom sklearn import linear_model\nfrom sklearn import datasets\n#import sklearn\n#from matplotlib import pyplot as plt\n#from sklearn.linear_model import LogisticRegression\n#import seaborn as sns\nsns.set()\n#from sklearn.model_selection import train_test_split\n#from sklearn.metrics import confusion_matrix\n#import pandas as pd\n\n\n#create a data set\nx, y = datasets.make_classification(\n    n_samples=100,\n    n_features=1,\n    n_classes=2,\n    n_clusters_per_class=1,\n    flip_y=0.03,\n    n_informative=1,\n    n_redundant=0,\n    n_repeated=0\n)\n\n#plt.scatter(x, y, c=y, cmap='rainbow')",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name __check_build",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-baba3a1a2d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#from sklearn.datasets import make_classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#import sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from matplotlib import pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/home/nbuser/anaconda2_501/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name __check_build"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}